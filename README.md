# Training

## Overview

This training program provides hands-on experience with two complementary technology stacks that form the foundation of modern automation and data processing workflows. Each module builds practical skills through real-world projects and comprehensive exercises.

## Training Modules

### Module 1: Python Development - Data Manipulation, Automation & CLI Applications

**Focus**: Building professional Python applications for data processing, web scraping, and command-line interfaces

**Key Learning Areas:**
- **Data Manipulation**: Advanced DataFrame operations with Pandas and NumPy
- **CLI Development**: Professional command-line applications using Typer and Rich
- **Web Scraping**: Automated data collection with BeautifulSoup and Requests
- **Date/Time Handling**: Modern datetime operations with Pendulum
- **File Operations**: Multi-format data processing (JSON, CSV, structured data)

**Core Technologies:**
- Pandas, NumPy for data analysis
- BeautifulSoup4, Requests for web scraping  
- Typer, Rich for CLI development
- Pendulum for datetime handling
- Plotille for terminal visualization

**Capstone Project**: Weather CLI Scraper - A comprehensive weather application demonstrating real-time data collection, interactive CLI interfaces, statistical analysis, and multi-format data export.

**Supporting Projects**:
- **News Scraper**: Automated headline collection with duplicate detection
- **Task Manager CLI**: Interactive productivity tool with rich terminal formatting

---

### Module 2: Shell Scripting & Automation - System Administration & Data Pipelines

**Focus**: System administration, workflow automation, and data processing pipeline development

**Key Learning Areas:**
- **Shell Scripting Fundamentals**: Variables, loops, conditionals, and command substitution
- **System Automation**: File operations, backup strategies, and system monitoring
- **Task Scheduling**: Cron job configuration and automated workflow management
- **Data Pipeline Architecture**: ETL vs ELT implementation patterns
- **Python Integration**: Combining shell scripts with Python for comprehensive solutions

**Core Technologies:**
- Bash scripting and system utilities
- Cron for job scheduling
- Python integration for data transformation
- File system operations and process management

**Pipeline Projects**:
- **ETL Pipeline**: Extract-Transform-Load workflow with data validation
- **ELT Pipeline**: Extract-Load-Transform for scalable data processing
- **System Utilities**: Information display, file organization, and calculation tools

**Key Distinction - ETL vs ELT**:
- **ETL**: Transform data before loading (better for smaller datasets, data validation)
- **ELT**: Load data first, then transform (better for large datasets, leverages target system power)




